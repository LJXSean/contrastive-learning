{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data points:\n",
      "{'source': 'explicit', 'citeEnd': 175, 'sectionName': 'Introduction', 'citeStart': 168, 'string': 'However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).', 'label': 'background', 'label_confidence': 1.0, 'citingPaperId': '1872080baa7d30ec8fb87be9a65358cd3a7fb649', 'citedPaperId': '894be9b4ea46a5c422e81ef3c241072d4c73fdc0', 'isKeyCitation': True, 'id': '1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be9b4ea46a5c422e81ef3c241072d4c73fdc0', 'unique_id': '1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be9b4ea46a5c422e81ef3c241072d4c73fdc0_11', 'excerpt_index': 11}\n",
      "{'source': 'explicit', 'citeStart': 16, 'sectionName': 'Novel Quantitative Trait Loci for Seminal Root Traits in Barley', 'string': 'In the study by Hickey et al. (2012), spikes were sampled from the field at the point of physiological\\nrobinson et al.: genomic regions influencing root traits in barley 11 of 13\\nmaturity, dried, grain threshed by hand, and stored at âˆ’20C to preserve grain dormancy before germination testing.', 'citeEnd': 36, 'label': 'background', 'label_confidence': 1.0, 'citingPaperId': 'ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b', 'citedPaperId': 'b6642e19efb8db5623b3cc4eef1c5822a6151107', 'isKeyCitation': True, 'id': 'ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b>b6642e19efb8db5623b3cc4eef1c5822a6151107', 'unique_id': 'ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b>b6642e19efb8db5623b3cc4eef1c5822a6151107_2', 'excerpt_index': 2}\n",
      "{'source': 'explicit', 'citeEnd': 228, 'sectionName': 'Introduction', 'citeStart': 225, 'string': 'The drug also reduces catecholamine secretion, thereby reducing stress and leading to a modest (10-20%) reduction in heart rate and blood pressure, which may be particularly beneficial in patients with cardiovascular disease.(7) Unlike midazolam, dexmedetomidine does not affect the ventilatory response to carbon dioxide.', 'label': 'background', 'label_confidence': 1.0, 'citingPaperId': '9cdf605beb1aa1078f235c4332b3024daa8b31dc', 'citedPaperId': '4e6a17fb8d7a3cada601d942e22eb5da6d01adbd', 'isKeyCitation': False, 'id': '9cdf605beb1aa1078f235c4332b3024daa8b31dc>4e6a17fb8d7a3cada601d942e22eb5da6d01adbd', 'unique_id': '9cdf605beb1aa1078f235c4332b3024daa8b31dc>4e6a17fb8d7a3cada601d942e22eb5da6d01adbd_0', 'excerpt_index': 0}\n",
      "Keys: ['source', 'citeEnd', 'sectionName', 'citeStart', 'string', 'label', 'label_confidence', 'citingPaperId', 'citedPaperId', 'isKeyCitation', 'id', 'unique_id', 'excerpt_index']\n",
      "Number of training data points: 8243\n",
      "Label distribution: {'background': 4840, 'method': 2294, 'result': 1109}\n"
     ]
    }
   ],
   "source": [
    "file_path_train = 'scicite/train.jsonl'\n",
    "file_path_dev = 'scicite/dev.jsonl'\n",
    "file_path_test = 'scicite/test.jsonl'\n",
    "train_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "with open(file_path_train, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        train_data.append(json.loads(line))\n",
    "with open(file_path_dev, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        dev_data.append(json.loads(line))\n",
    "with open(file_path_test, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        test_data.append(json.loads(line))\n",
    "print(\"Sample data points:\")\n",
    "print(train_data[0])\n",
    "print(train_data[1])\n",
    "print(train_data[2])\n",
    "print(\"Keys:\", list(train_data[0].keys()))\n",
    "print(\"Number of training data points:\", len(train_data))\n",
    "print(\"Label distribution:\", {x['label']: sum([1 for y in train_data if y['label'] == x['label']]) for x in train_data[:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CitationsDatasetWithoutInputExample():\n",
    "    label_to_id = {'background': 0, 'method': 1, 'result': 2}\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]['string'], CitationsDatasetWithoutInputExample.label_to_id[self.data[item]['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CitationsDatasetWithoutInputExample(train_data)\n",
    "train_batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = CitationsDatasetWithoutInputExample(dev_data)\n",
    "dev_batch_size = 32\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=dev_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training loss: 0.00430073479349299\n",
      "Evaluation loss: 0.037112844401392446\n",
      "Evaluation accuracy: 0.018558951965065504\n"
     ]
    }
   ],
   "source": [
    "class CitationIntentClassifier(nn.Module):\n",
    "    def __init__(self, model_path, num_labels):\n",
    "        super(CitationIntentClassifier, self).__init__()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        self.sentence_transformer = RobertaModel.from_pretrained(model_path)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_texts):\n",
    "        tokenised = self.tokenizer(input_texts, return_tensors='pt', truncation=True, padding='max_length', max_length=256)\n",
    "        embeddings = self.sentence_transformer(**tokenised)\n",
    "        cls_representation = embeddings.last_hidden_state[:, 0]\n",
    "        return self.classifier(cls_representation)\n",
    "    \n",
    "model_path = 'roberta-base'\n",
    "num_labels = 3\n",
    "citation_intent_classifier = CitationIntentClassifier(model_path, num_labels)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(citation_intent_classifier.parameters(), lr=learning_rate)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch(model, dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_texts, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_texts)\n",
    "        loss = loss_func(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Training loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "def evaluate(model, dataloader, loss_func):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input_texts, labels in dataloader:\n",
    "            output = model(input_texts)\n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (output.argmax(1) == labels).sum().item()\n",
    "            \n",
    "    print(f\"Evaluation loss: {total_loss / len(dataloader)}\")\n",
    "    print(f\"Evaluation accuracy: {total_correct / len(dataloader.dataset)}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_epoch(citation_intent_classifier, train_dataloader, loss_func, optimizer)\n",
    "    evaluate(citation_intent_classifier, dev_dataloader, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CitationsDatasetWithoutInputExample(test_data)\n",
    "test_batch_size = 32\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_texts, labels in dataloader:\n",
    "            output = model(input_texts)\n",
    "            _, predicted_labels = torch.max(output, dim=1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return predictions, true_labels\n",
    "\n",
    "predictions, true_labels = test(citation_intent_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.2127659574468085\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(true_labels, predictions, average='macro')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
