{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = 'scicite/train.jsonl'\n",
    "file_path_dev = 'scicite/dev.jsonl'\n",
    "file_path_test = 'scicite/test.jsonl'\n",
    "train_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "with open(file_path_train, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        train_data.append(json.loads(line))\n",
    "with open(file_path_dev, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        dev_data.append(json.loads(line))\n",
    "with open(file_path_test, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        test_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CitationsDatasetWithoutInputExample():\n",
    "    label_to_id = {'background': 0, 'method': 1, 'result': 2}\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]['string'], CitationsDatasetWithoutInputExample.label_to_id[self.data[item]['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CitationsDatasetWithoutInputExample(train_data)\n",
    "train_batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = CitationsDatasetWithoutInputExample(dev_data)\n",
    "dev_batch_size = 32\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=dev_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "('However, luciferase gene expression from the IFN-stimulated Mx1 promoter or the SV40 promoter was reduced by PR(1-80)Tx(80-239) but not by Tx(1-80)PR(81-230) (Fig.', 'However, luciferase gene expression from the IFN-stimulated Mx1 promoter or the SV40 promoter was reduced by PR(1-80)Tx(80-239) but not by Tx(1-80)PR(81-230) (Fig.')\n"
     ]
    }
   ],
   "source": [
    "class CitationIntentClassifier(nn.Module):\n",
    "    def __init__(self, model_path, num_labels):\n",
    "        super(CitationIntentClassifier, self).__init__()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        self.sentence_transformer = RobertaModel.from_pretrained(model_path)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_texts):\n",
    "        tokenised = self.tokenizer(input_texts, return_tensors='pt', truncation=True, padding='max_length', max_length=256)\n",
    "        embeddings = self.sentence_transformer(**tokenised)\n",
    "        cls_representation = embeddings.last_hidden_state[:, 0]\n",
    "        return self.classifier(cls_representation)\n",
    "\n",
    "def train_epoch(model, dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_texts, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_texts)\n",
    "        loss = loss_func(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Training loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "def evaluate(model, dataloader, loss_func):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input_texts, labels in dataloader:\n",
    "            output = model(input_texts)\n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (output.argmax(1) == labels).sum().item()\n",
    "            \n",
    "    print(f\"Evaluation loss: {total_loss / len(dataloader)}\")\n",
    "    print(f\"Evaluation accuracy: {total_correct / len(dataloader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CitationsDatasetWithoutInputExample(test_data)\n",
    "test_batch_size = 32\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_texts, labels in dataloader:\n",
    "            output = model(input_texts)\n",
    "            _, predicted_labels = torch.max(output, dim=1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return predictions, true_labels\n",
    "\n",
    "\n",
    "def train_test_loop(model_name):\n",
    "    num_labels = 3\n",
    "    citation_intent_classifier = CitationIntentClassifier(model_name, num_labels)\n",
    "\n",
    "    # Parameters\n",
    "    learning_rate = 2e-5\n",
    "    num_epochs = 5\n",
    "\n",
    "    optimizer = torch.optim.Adam(citation_intent_classifier.parameters(), lr=learning_rate)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        train_epoch(citation_intent_classifier, train_dataloader, loss_func, optimizer)\n",
    "        evaluate(citation_intent_classifier, dev_dataloader, loss_func)\n",
    "        \n",
    "    predictions, true_labels = test(citation_intent_classifier, test_dataloader)\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.2127659574468085\n"
     ]
    }
   ],
   "source": [
    "models = ['roberta-base', 'largest']\n",
    "for model in models:\n",
    "    train_test_loop(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
