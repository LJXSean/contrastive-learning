{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data_path_aclarc = \"./acl-arc/scaffolds/sections-scaffold-train.jsonl\"\n",
    "data_path_scicite = \"./scicite/scaffolds/sections-scaffold-train.jsonl\"\n",
    "with open(data_path_aclarc, encoding='utf-8') as data_file:\n",
    "    data = [json.loads(line) for line in data_file]\n",
    "    df = pd.DataFrame(data).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_cols_section_paper = ['section_name', 'cited_paper_id']\n",
    "sort_cols_section = ['section_name']\n",
    "sort_cols = sort_cols_section_paper\n",
    "final_cols = ['text', 'text_pos', 'section_name', 'citing_paper_id', 'cited_paper_id']\n",
    "\n",
    "def split_and_concatenate(group):\n",
    "    # Calculate the split index\n",
    "    split_index = len(group) // 2\n",
    "    \n",
    "    # Split the group into two halves\n",
    "    first_half = group.iloc[:split_index].reset_index(drop=True)['text']\n",
    "    second_half = group.iloc[split_index:].reset_index(drop=True)\n",
    "    second_half.rename(columns={'text': 'text_pos'}, inplace=True)\n",
    "\n",
    "    # Concatenate the halves horizontally\n",
    "    concatenated = pd.concat([first_half, second_half], axis=1)\n",
    "    return concatenated\n",
    "\n",
    "# Gets samples using concatenation\n",
    "def get_pos_samples_concat(df):\n",
    "    df_concat = df.copy(deep=True)\n",
    "\n",
    "    # Dummy columns for groupby, to keep original columns\n",
    "    include_groups = [i + '_drop' for i in sort_cols]\n",
    "    df_concat[include_groups] = df_concat[sort_cols]\n",
    "    \n",
    "    result = df_concat.groupby(include_groups).apply(split_and_concatenate, include_groups=False).reset_index(drop=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets samples using Right Outer join\n",
    "def get_pos_samples_rj(df):\n",
    "    df_sorted = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "    df_sorted['WithinGroupID'] = df_sorted.groupby(sort_cols).cumcount()\n",
    "\n",
    "    # Calculate the size of each group and the split point\n",
    "    df_sorted['group_sizes'] = df_sorted.groupby(sort_cols)['WithinGroupID'].transform('max') + 1\n",
    "    df_sorted['cutoff'] = (df_sorted['group_sizes'] / 2).round().astype(int)\n",
    "\n",
    "\n",
    "    # Split groups into half\n",
    "    first_half = df_sorted[df_sorted['WithinGroupID'] < df_sorted['cutoff']]\n",
    "    second_half = df_sorted[df_sorted['WithinGroupID'] >=  df_sorted['cutoff']].reset_index(drop=True)\n",
    "\n",
    "    # Right Join on second half\n",
    "    second_half = second_half.set_axis([i + '_pos' for i in second_half.columns], axis=1)\n",
    "    result = first_half.merge(second_half, how='right', left_on=sort_cols, right_on=[i + '_pos' for i in sort_cols])\n",
    "\n",
    "    selected_cols = ['text', 'text_pos', 'section_name_pos', 'citing_paper_id_pos', 'cited_paper_id_pos']\n",
    "    result = result[selected_cols]\n",
    "    \n",
    "    return result.set_axis(final_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = get_pos_samples_concat(df)\n",
    "rj = get_pos_samples_rj(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NA with text_pos (dropout in roberta will treat this as unsupervised learning)\n",
    "def handle_na(df):\n",
    "    df.loc[pd.isna(df['text']), 'text'] = df.loc[pd.isna(df['text'])]['text_pos']\n",
    "\n",
    "handle_na(concat)\n",
    "handle_na(rj)\n",
    "\n",
    "concat[['text', 'text_pos']].to_csv('data_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"data_file.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "column_names = dataset['train'].column_names\n",
    "\n",
    "# Testing\n",
    "def tokenize(examples, max_length=256):\n",
    "    id_masks_all_cols = []\n",
    "    col_name = column_names[0]\n",
    "    length = len(examples[col_name])\n",
    "\n",
    "    # Tokenize examples for each column\n",
    "    for k in column_names:\n",
    "        id_mask = tokenizer(examples[k], truncation=True, padding='max_length', max_length=max_length)\n",
    "        id_masks_all_cols.append(id_mask)\n",
    "\n",
    "    zipped_id_mask = {}\n",
    "    id_mask_col = id_masks_all_cols[0]\n",
    "\n",
    "    # Zips all columns together for each feature, input_id/attention_mask\n",
    "    for feature in id_mask_col:\n",
    "        zipped_id_mask[feature] = [[id_mask[feature][i] for id_mask in id_masks_all_cols] for i in range(length)]\n",
    "\n",
    "    return zipped_id_mask\n",
    "\n",
    "# Shape = [#features, #sentences, #samples(anchor, pos, neg)]\n",
    "tokenized = dataset['train'].map(tokenize, batched=True, remove_columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create batch of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized.set_format(\"torch\")\n",
    "\n",
    "train_size = int(0.8 * len(tokenized))\n",
    "test_size = len(tokenized) - train_size\n",
    "\n",
    "small_train_dataset = tokenized.shuffle(seed=42).select(range(train_size))\n",
    "test_dataset = tokenized.shuffle(seed=42).select(range(train_size, train_size+test_size))\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NT-Xent Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def contrastive_loss(embeddings, temperature=0.1, train=True):\n",
    "    sents_per_vector = embeddings.size(1)\n",
    "\n",
    "    if sents_per_vector < 2 or sents_per_vector > 3:\n",
    "        raise Exception(\"Unexpected number of sentences per sample received. Expected: 2/3\") \n",
    "    \n",
    "    cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    # Reshape to 3D for broadcast computation\n",
    "    anchor = embeddings[:, 0].unsqueeze(1)\n",
    "    positive = embeddings[:, 1].unsqueeze(0)\n",
    "    \n",
    "    # Pairwise cosine similarity, shape = [batch_size, batch_size]\n",
    "    pairwise_sim = cos_sim(anchor, positive)\n",
    "\n",
    "    # index of positive sample for corresponding anchors (matrix diagonal)\n",
    "    target = torch.arange(pairwise_sim.size(0))\n",
    "\n",
    "    # Horizontally concatenate hard_neg similarities (if any)\n",
    "    if sents_per_vector == 3:\n",
    "        hard_neg = embeddings[:, 2].unsqueeze(0)\n",
    "        hard_neg_sim = cos_sim(anchor, hard_neg)\n",
    "        pairwise_sim = torch.cat([pairwise_sim, hard_neg_sim], 1)\n",
    "    \n",
    "    pairwise_sim /= temperature\n",
    "\n",
    "    if train:\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        output = loss(pairwise_sim, target)\n",
    "\n",
    "        return output\n",
    "    else:\n",
    "        predicted = torch.argmax(pairwise_sim, dim=1)\n",
    "        return predicted, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import RobertaModel\n",
    "\n",
    "def encoder(batch, model):\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    batch_size, sents_per_vector, tensor_size = input_ids.shape\n",
    "\n",
    "    # Flatten to encode all at once\n",
    "    input_ids = torch.reshape(input_ids, (-1, tensor_size))\n",
    "    attention_mask = torch.reshape(attention_mask, (-1, tensor_size))\n",
    "\n",
    "    # Use [CLS] token representation\n",
    "    # data augmentation handled by roberta, dropout implemented under the hood\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    embeddings = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "    # Add dropout layer for better performance?\n",
    "\n",
    "    # Reshape back to nested tensors\n",
    "    embeddings = torch.reshape(embeddings, (batch_size, sents_per_vector, -1))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    # Shape = [#features, #batch_size, #tensor_length]\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeddings = encoder(batch, model)\n",
    "        loss = contrastive_loss(embeddings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch: {i+1}/{len(train_dataloader)}, Loss: {total_loss/(i+1)}\")\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader)}\")\n",
    "\n",
    "save_directory = './pretrained'  # Specify your save directory\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "def evaluate(data_loader, model):\n",
    "    y_pred, y_test = [], []\n",
    "    model.eval()\n",
    "\n",
    "    f1_metric = load_metric('f1')\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            outputs = encoder(batch, model)\n",
    "            \n",
    "        y_pred_batch, y_batch = contrastive_loss(outputs, train=False)\n",
    "        y_test += list(y_batch.detach().numpy())\n",
    "        y_pred += list(y_pred_batch.detach().numpy())\n",
    "\n",
    "        print(y_pred_batch, y_batch)\n",
    "\n",
    "        f1_metric.add_batch(predictions=y_pred_batch, references=y_batch)\n",
    "        if i == 5:\n",
    "            break\n",
    "    \n",
    "    return f1_metric.compute(average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sean2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  8, 15,  5,  6,  7, 24,  6, 10, 11,  7, 13, 23, 15, 23, 17,\n",
      "        10, 19, 20, 21,  3, 20, 24, 25,  6, 27, 15, 29,  8, 31]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([ 0,  1,  2,  3,  4, 18, 16,  7, 15,  9, 10, 11, 12,  3, 14, 15, 16, 17,\n",
      "        18, 19,  4,  4, 22, 23, 24, 25, 26,  6, 28, 29, 16, 18]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([ 0, 12,  2,  0,  4,  5,  6,  7,  5, 30, 20, 11, 12, 13, 12, 15, 16, 17,\n",
      "        18, 17, 28, 21, 22, 23, 24, 25, 26, 27, 28, 18, 30, 31]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([ 0,  1, 18, 13,  4,  5,  6,  7,  8, 28, 10, 18, 12, 13, 14, 15,  5, 17,\n",
      "        18, 19, 30, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([16,  1, 24,  3, 24,  5, 24,  4, 24,  9, 24, 11, 12, 13, 14,  4, 16, 17,\n",
      "        18, 19, 11, 21, 17, 23, 24, 25, 26, 27, 28, 24, 30, 31]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([ 0,  1,  2, 23,  4,  5,  6, 30,  1,  9, 10, 11, 12, 13, 14,  4, 16, 17,\n",
      "         1, 19, 20,  1, 22, 23, 24, 25, 26, 27, 28, 29, 30, 21]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "f1 for roberta-base: {'f1': 0.7140990770924982}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2, 20,  4,  5,  6,  7,  7,  9, 10, 11, 12,  9,  0, 15,  4, 17,\n",
      "        18, 18, 20,  8, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([ 0,  1,  2,  3, 27,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         8, 19, 20, 21, 22, 23, 24, 25,  3, 27,  3, 29, 30, 31]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([25,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 21, 16, 17,\n",
      "        16, 19, 20, 21, 22, 17, 24, 25, 17, 27, 28, 29, 14, 16]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 31,  8, 11, 12, 13, 14, 15,  0, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24,  8, 26, 27, 28, 28, 30, 28]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([ 0,  1,  2, 22,  4,  5,  8,  7,  8,  9, 10, 11, 12, 13, 11, 15, 16, 17,\n",
      "        20, 17, 13, 21, 22, 23, 21, 25, 26, 25, 24, 15, 22, 31]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([ 0,  1, 12,  3, 17, 19,  6,  7,  8,  9, 31, 11, 12, 13, 19, 15, 19, 17,\n",
      "        19, 19, 20, 12, 22, 19, 24, 25, 26, 27, 28, 29, 30, 31]) tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "f1 for ./pretrained: {'f1': 0.7704450931013431}\n"
     ]
    }
   ],
   "source": [
    "model_names = ['roberta-base', './pretrained']\n",
    "\n",
    "for name in model_names:\n",
    "    model = RobertaModel.from_pretrained(name)\n",
    "    f1 = evaluate(test_dataloader, model)\n",
    "    print(f'f1 for {name}: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.6807e-02,  8.1177e-02, -1.0582e-02, -1.4226e-01,  5.8871e-02,\n",
      "         -1.1665e-01, -1.6456e-02,  2.3507e-02,  7.6793e-02, -3.8536e-02,\n",
      "         -2.0918e-02,  4.6355e-02,  3.7463e-02, -5.1474e-02,  6.1221e-02,\n",
      "          2.5821e-02, -9.9584e-02, -1.1229e-02,  9.0533e-04, -3.7513e-02,\n",
      "         -9.6354e-02,  6.0888e-02, -4.3323e-02,  1.0892e-01, -1.0954e-02,\n",
      "          4.6057e-02,  8.5546e-02,  6.1048e-02, -6.7207e-02, -1.8888e-03,\n",
      "         -2.4986e-02, -4.2359e-02,  5.6012e-02, -5.0210e-02,  3.4153e-02,\n",
      "          8.0668e-02,  3.2120e-02, -1.4985e-02, -9.2031e-02, -6.0763e-03,\n",
      "          2.5994e-03,  5.9213e-02, -5.8606e-03, -3.4089e-03,  6.7560e-02,\n",
      "          3.3174e-03,  1.6809e-02,  5.4562e-02, -3.3795e-02,  3.0461e-02,\n",
      "         -2.6839e-03,  9.2151e-02, -3.4292e-02,  1.3671e-02, -8.9881e-02,\n",
      "          1.9958e-02, -1.5046e-03,  1.0379e-01,  7.3402e-02, -4.9641e-02,\n",
      "         -5.9235e-03, -1.0554e-01, -1.1929e-01, -4.0894e-02,  8.7989e-03,\n",
      "         -1.3469e-02, -4.9082e-02,  2.9597e-03,  1.5049e-02,  6.3068e-02,\n",
      "          7.1486e-02, -3.1219e-02,  5.1370e-02, -3.3114e-02, -1.3903e-02,\n",
      "          1.5762e-02, -1.0533e-03,  5.8101e-01, -3.5647e-02,  9.2038e-03,\n",
      "          4.8133e-02, -2.4702e-02,  4.2012e-01,  3.3041e-02,  1.5670e-02,\n",
      "          3.4603e-02,  8.3110e-02,  4.7646e-02,  1.7507e-02,  1.8034e-02,\n",
      "          4.7975e-02,  4.5943e-02, -6.1842e-02, -3.9044e-03,  2.5449e-02,\n",
      "          5.5985e-02, -2.3960e-02,  2.0214e-02, -1.5477e-02, -8.2427e-02,\n",
      "         -3.3032e-02, -6.1877e-02,  1.0540e-01,  5.3631e-02,  1.6473e-02,\n",
      "          2.0573e-02,  2.1594e-02, -2.2517e-02,  3.7141e-02, -2.0708e-02,\n",
      "          3.2069e-02,  4.2882e-02,  5.6857e-03,  5.9901e-02, -2.5638e-02,\n",
      "         -3.1854e-02, -3.2697e-02, -3.9481e-04, -1.3523e-03,  4.0102e-02,\n",
      "          3.6440e-02,  7.5213e-02,  1.2198e-01, -4.2733e-02, -5.2027e-02,\n",
      "         -4.2518e-02, -3.2621e-02, -7.1750e-03, -6.1358e-02,  3.3375e-02,\n",
      "          2.0172e-02, -7.7329e-02,  7.1719e-03,  9.4807e-02,  6.2181e-02,\n",
      "          3.2521e-02,  5.9258e-02, -4.1667e-02,  6.3544e-02, -1.8122e-02,\n",
      "         -5.0745e-02,  7.4672e-02,  8.9016e-02,  2.3068e-02,  9.2688e-02,\n",
      "          3.5891e-03, -9.9318e-03, -3.8205e-02,  5.9030e-02, -1.4936e-02,\n",
      "          4.9420e-02, -7.3188e-02, -4.7745e-02,  1.4629e-02, -2.2657e-02,\n",
      "          4.4419e-01,  9.1876e-02,  4.8726e-02, -9.5194e-03,  3.5698e-02,\n",
      "          1.5042e-01, -1.3949e-02,  2.7241e-02, -4.4166e-03, -4.4060e-02,\n",
      "          1.8186e-02, -3.2962e-03,  3.4635e-02,  3.6067e-02, -1.4344e-03,\n",
      "          6.3020e-02,  4.8869e-02,  1.6783e-02, -8.1485e-02, -3.5007e-02,\n",
      "         -7.3097e-02, -2.0996e-03, -1.4131e-02, -1.0286e-01, -4.6443e-03,\n",
      "          6.4237e-02,  8.1514e-02, -7.0724e-02,  2.6968e-02, -6.1261e-02,\n",
      "          2.4381e-02,  2.2162e-03,  2.3703e-02, -1.2260e-02,  2.0501e-02,\n",
      "          3.7707e-02, -3.2920e-02,  6.9110e-03, -8.6747e-05, -7.7865e-03,\n",
      "          7.8889e-02, -2.2202e-02, -3.7713e-02,  5.4759e-02, -5.5022e-02,\n",
      "          7.4989e-02, -8.6104e-02,  1.2535e-01, -1.1567e-01,  4.6775e-02,\n",
      "         -2.0989e-02,  5.2221e-02,  6.7390e-02,  1.7065e-02, -4.6526e-02,\n",
      "         -6.2548e-02,  4.5589e-02, -1.3451e-02,  8.6825e-02,  1.9302e-02,\n",
      "          8.4187e-03,  5.7236e-02,  1.3629e-01, -1.6263e-02, -1.2539e-02,\n",
      "          3.8119e-02,  4.9100e-02,  3.0549e-03,  8.3545e-02,  1.9000e-02,\n",
      "         -1.4576e-02,  1.7173e-02, -2.0255e-02,  3.8461e-02, -4.0105e-02,\n",
      "         -1.8787e-02,  6.4268e-02,  4.2319e-02, -3.2684e-02,  6.7520e-02,\n",
      "         -1.2748e-01, -9.5313e-04, -4.5535e-02, -4.4542e-02,  2.7772e-02,\n",
      "         -1.0389e-01,  7.1139e-02,  3.3542e-02,  1.0205e-02,  2.0417e-02,\n",
      "          5.2240e-02, -1.2972e-03,  9.5642e-02, -2.9879e-02,  4.8243e-02,\n",
      "         -1.6836e-02,  1.8536e-03, -2.1586e-02, -3.6211e-02,  3.1436e-02,\n",
      "         -3.3249e-02, -1.2123e-01, -1.2560e-02,  2.0532e-02, -2.0136e-02,\n",
      "          2.9755e-04, -6.5722e-02,  8.7397e-03, -2.7054e-02, -1.2915e-01,\n",
      "         -5.6537e-02, -2.8713e-03,  5.6515e-02,  3.8470e-02, -5.8742e-02,\n",
      "         -2.4244e-02, -5.1086e-02,  1.4624e-02,  7.4397e-03,  1.3641e-02,\n",
      "          4.3469e-02, -3.5583e-02, -2.4096e-02, -7.3053e-02,  1.2008e-03,\n",
      "          4.1915e-02, -3.2425e-02, -7.1674e-02, -2.3019e-02, -2.3496e-02,\n",
      "          3.7417e-02, -4.3381e-02, -7.2891e-02,  2.5193e-02,  5.1815e-02,\n",
      "          6.3985e-02,  3.5155e-02, -2.1285e-02,  1.5421e-02, -5.9360e-02,\n",
      "          4.5858e-02,  9.9742e-02,  1.8757e-03,  1.3488e-02,  2.5498e-02,\n",
      "         -7.2066e-02, -7.9991e-02, -8.6639e-02,  1.4148e-02, -1.5801e-02,\n",
      "         -9.0361e-02,  1.6114e-04, -5.7525e-02,  8.2714e-02, -1.2088e-02,\n",
      "         -5.0626e-02, -2.9544e-02, -1.0006e-01,  1.6539e-01,  4.7842e-02,\n",
      "         -1.1902e-02,  8.2764e-02, -5.7190e-02,  2.6061e-02,  4.9185e-02,\n",
      "         -6.1286e-03,  6.1961e-03,  1.0376e-01, -8.8927e-03, -2.8979e-02,\n",
      "          2.7115e-02,  4.8158e-02, -1.8471e-02,  3.6653e-02,  3.9617e-01,\n",
      "         -3.4770e-01,  1.7979e-03,  6.3584e-02,  2.9022e-02,  7.8165e-02,\n",
      "         -1.5881e-02,  6.3935e-02,  3.0453e-02,  7.8723e-02,  7.1591e-02,\n",
      "         -4.1108e-02,  6.3969e-02, -1.1544e-02,  4.5368e-02,  2.1641e-02,\n",
      "          2.4966e-02, -3.7700e-02, -1.3061e-02, -4.4737e-03,  1.4346e-02,\n",
      "          1.2511e-02, -3.5956e-02,  4.3044e-02, -2.8065e-02, -2.0340e-02,\n",
      "          4.6374e-02,  3.1500e-02, -2.1364e-02, -2.5121e-02,  1.4412e-02,\n",
      "          6.3212e-02,  2.1029e-02,  3.1309e-02,  5.6832e-03,  7.9109e-02,\n",
      "         -7.1488e-02,  4.0483e-03,  6.4297e-02,  2.4439e-02,  3.1063e-02,\n",
      "          9.3804e-02, -5.3566e-02, -2.5462e-02,  5.2193e-03,  2.0565e-02,\n",
      "         -7.4757e-03,  7.1551e-02, -1.6926e-02, -2.3560e-02,  1.0881e-01,\n",
      "          3.5187e-02,  2.3761e-02, -5.5207e-02, -1.5973e-02,  1.0497e-01,\n",
      "         -7.4248e-02,  2.9040e-02,  8.9810e-03,  6.9507e-02,  1.0875e-02,\n",
      "         -2.2626e-02, -9.6801e-02, -4.8476e-02, -9.4418e-02,  1.1032e-01,\n",
      "         -2.1969e-02, -7.8632e-02, -1.8180e-01, -3.4263e-02, -6.0463e-02,\n",
      "          3.3629e-02,  5.0357e-02, -7.6711e-02,  1.6340e-02,  1.1246e-02,\n",
      "          1.8314e-02,  2.4370e-02, -6.1200e-02, -3.0366e-03, -4.0507e-02,\n",
      "         -2.2604e-02,  4.1952e-02,  5.1089e-02, -1.7816e-02,  2.1787e-02,\n",
      "         -2.7644e-02,  3.5637e-02, -4.4438e-04, -2.3961e-02, -6.9845e-04,\n",
      "         -5.1198e-02,  8.2771e-02,  3.0197e-02,  2.1167e-02, -3.2310e-02,\n",
      "          1.0760e-02,  3.7243e-02, -7.1515e-02, -4.0292e-02, -7.1074e-03,\n",
      "         -1.0548e-02,  7.8799e-02, -6.3252e-02, -2.9459e-02,  9.9186e-03,\n",
      "         -2.4860e-02,  3.3731e-02, -1.7023e-02, -1.7824e-02, -5.4092e-02,\n",
      "         -3.1755e-02, -2.1331e-02,  4.3685e-02,  9.3872e-03, -7.1047e-02,\n",
      "         -9.6216e-03,  9.3594e-02, -2.8025e-03, -1.0460e-01,  6.2441e-03,\n",
      "          5.0400e-02,  4.3323e-02, -4.5559e-02, -6.0568e-01,  2.4605e-02,\n",
      "          2.7855e-02,  1.3345e-02,  3.2631e-02, -7.6091e-02,  8.6650e-03,\n",
      "          3.8665e-02, -8.3786e-03,  4.1986e-02, -5.4413e-02,  1.8420e-02,\n",
      "          2.1583e-02, -6.5485e-02,  3.3029e-02, -8.2257e-02, -3.6967e-02,\n",
      "          6.6522e-02, -5.5831e-03, -2.9648e-02, -6.6305e-02,  1.8510e-02,\n",
      "         -2.4860e-02, -1.4339e-02,  7.3561e-02, -2.7805e-02, -1.1300e-01,\n",
      "         -2.7788e-02,  7.6015e-02,  5.7243e-02,  2.2199e-03, -6.7336e-02,\n",
      "         -3.5420e-02, -2.1910e-02,  3.4550e-02,  7.5318e-02, -3.5712e-04,\n",
      "         -1.4638e-03, -5.3411e-02,  2.7317e-02,  2.3309e-02,  2.1925e-01,\n",
      "         -1.4950e-02,  2.3859e-01, -2.0420e-03,  1.7571e-02, -1.4999e-02,\n",
      "         -4.4800e-02, -3.0310e-02,  1.5988e-02,  1.4310e-02, -6.3847e-02,\n",
      "          9.1651e-03, -8.9533e-02, -3.7189e-02,  3.8666e-03,  1.5472e-02,\n",
      "         -4.4635e-02,  1.7764e-02,  1.5209e-01, -4.0589e-03,  3.7336e-02,\n",
      "          1.7777e-02, -3.1501e-02, -1.9752e-02, -4.4348e-02,  1.8343e-03,\n",
      "         -7.4756e-02,  3.6531e-02, -1.7178e-02, -6.4414e-02, -8.0897e-02,\n",
      "         -4.0244e-02,  3.7348e-02,  1.2488e-02,  9.5153e-02,  1.0530e-02,\n",
      "          1.0996e-02, -1.9662e-02,  5.9320e-02, -1.6065e-02,  4.7896e-02,\n",
      "          4.6410e-02,  7.1075e-02,  6.6022e-02, -7.0795e-02, -3.3889e-02,\n",
      "         -2.4020e-02,  6.1738e-02,  1.1567e-01, -1.9015e-02,  1.0815e-01,\n",
      "          3.6723e-02, -4.1736e-03, -3.5665e-02,  4.4195e-02,  3.3387e-02,\n",
      "         -3.0533e-02, -7.0880e-01, -4.2871e-02,  4.4899e-02,  2.3140e-02,\n",
      "          1.3942e-02,  7.5921e-02, -7.5613e-03, -2.6847e-02,  4.3082e-02,\n",
      "         -4.6966e-02,  2.9899e-02,  7.4382e-03,  9.3083e-02, -7.0109e-02,\n",
      "          1.1224e-02,  6.5043e-02, -1.0788e-02, -5.3493e-02,  1.7645e-02,\n",
      "         -2.5483e-01, -2.7141e-02, -5.3101e-02,  8.6558e-02,  1.4765e-02,\n",
      "          2.9051e-02,  7.6732e-02, -4.9509e-02,  2.5124e-02,  5.0326e-02,\n",
      "          4.3951e-02,  6.4706e-02,  3.2535e-02, -2.0320e-02,  1.7640e-02,\n",
      "          1.2043e-02,  9.5498e-02, -1.2694e-02,  1.1063e+01, -1.7970e-02,\n",
      "          6.3077e-02, -4.1576e-03,  1.1154e-02, -7.0524e-02,  7.9829e-02,\n",
      "         -5.2772e-02, -1.7137e-02,  7.1119e-02,  1.4652e-02,  2.9666e-03,\n",
      "         -5.4661e-02, -3.0086e-02,  3.5805e-02,  2.8627e-02, -4.7028e-02,\n",
      "         -3.2827e-02,  5.9765e-02, -1.5824e-02, -3.3978e-02,  2.5662e-02,\n",
      "          2.1205e-02,  7.4800e-02, -5.9872e-02, -5.2154e-04,  4.2613e-02,\n",
      "         -2.1744e-02, -6.7399e-03,  7.4665e-03, -6.0728e-03,  2.3938e-02,\n",
      "          6.5024e-02, -1.5288e-02,  5.3562e-02,  4.1814e-02, -6.4890e-03,\n",
      "          6.4960e-02,  1.3107e-02,  7.5440e-02,  8.0193e-02, -3.8309e-03,\n",
      "         -4.2485e-03, -5.2015e-02,  6.9340e-02,  2.7326e-02, -3.6919e-02,\n",
      "          6.2117e-02,  7.6511e-03,  5.0816e-02,  7.6496e-02, -7.4683e-02,\n",
      "          4.8511e-02,  4.4980e-02, -2.3471e-02,  3.2891e-02,  2.8642e-02,\n",
      "         -2.0628e-02,  4.5881e-02,  4.4842e-02, -4.1234e-02,  1.0376e-01,\n",
      "         -1.5148e-02,  4.2680e-02, -5.2520e-02,  8.4159e-02,  8.0332e-02,\n",
      "          1.2714e-01, -5.6962e-02, -1.7703e-02, -3.2138e-02, -5.2946e-02,\n",
      "         -4.2978e-02,  4.9743e-03,  1.1675e-02, -7.5035e-02,  4.5307e-04,\n",
      "          4.5614e-03,  8.6003e-03, -1.3991e-03,  5.1277e-02,  2.2824e-02,\n",
      "          1.0770e-03, -3.6199e-02,  9.6449e-02,  1.5414e-02,  2.1178e-02,\n",
      "          7.8618e-02,  6.9559e-03,  1.8874e-02, -7.0829e-02,  2.8529e-02,\n",
      "         -1.6308e-02, -4.9374e-02,  2.5870e-02, -8.4814e-03,  1.3606e-02,\n",
      "         -9.8263e-02,  6.2220e-02,  7.0600e-02, -5.1904e-02,  2.5454e-02,\n",
      "          4.4553e-03,  6.4047e-02,  1.6161e-02,  5.0775e-02, -5.2484e-02,\n",
      "         -2.2354e-02,  4.8403e-02, -3.1804e-03, -3.1249e-02, -6.1171e-03,\n",
      "          2.0838e-03, -1.9415e-02,  1.8264e-02,  4.8372e-02,  7.6272e-03,\n",
      "          4.9676e-02,  8.7595e-03,  7.0964e-02, -7.2307e-02,  1.2303e-02,\n",
      "         -2.2503e-02, -3.5751e-02, -1.4056e-02, -3.4127e-02, -2.3064e-02,\n",
      "          8.4655e-03,  1.3278e-03, -1.2340e-02, -6.7926e-02,  7.1357e-03,\n",
      "          6.3750e-02, -1.1822e-02,  6.4964e-03, -3.1693e-02, -1.5512e-02,\n",
      "          9.4250e-02,  6.3081e-02,  4.3633e-02, -2.2239e-02, -1.1982e-02,\n",
      "         -3.8940e-02, -8.3778e-02, -2.6130e-02, -1.1089e-02,  6.9040e-02,\n",
      "          4.3625e-02,  8.2047e-02, -2.9368e-02,  3.9672e-02,  2.6869e-02,\n",
      "          7.8574e-03,  3.6314e-03,  4.0519e-02, -2.1747e-02, -1.3309e-02,\n",
      "          3.8936e-02,  5.5036e-03, -4.3289e-03,  4.0620e-02,  7.7236e-03,\n",
      "          3.5808e-02,  3.4940e-02,  1.1761e-02, -3.3386e-02, -1.3461e-02,\n",
      "          2.7293e-02,  2.7628e-02,  1.8311e-02,  2.6604e-02, -2.8013e-03,\n",
      "         -5.6140e-02, -5.4950e-02,  4.3448e-02,  1.1716e-01,  7.0448e-02,\n",
      "         -5.4440e-02, -6.6338e-02, -7.6022e-04]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Prepare the input text\n",
    "input_text = \"Hello, world!\"\n",
    "texts = [input_text, input_text]\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\")\n",
    "\n",
    "# Encode the input\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the sentence embedding\n",
    "sentence_embedding = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "print(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
